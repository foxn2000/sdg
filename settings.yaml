# --- vLLMの設定
Instruct_model_name: "/data/hf_data/model/Llama-3-ELYZA-JP-8B"
Instruct_model_quantization: null
Instruct_gpu_memory_utilization: 0.9
Instruct_dtype: "auto"
Instruct_temperature: 0.7
Instruct_top_p: 0.6

base_model_name: "/data/hf_data/model/sarashina2-70b"
base_model_quantization: null
base_gpu_memory_utilization: 0.7
base_dtype: "auto"
base_temperature: 0.65
base_top_p: 0.6

think_model_name: "/data/hf_data/model/cogito-v1-preview-llama-8B"
think_model_quantization: null
think_gpu_memory_utilization: 0.9
think_dtype: "auto"
think_temperature: 0.65
think_top_p: 0.6

tensor_parallel_size: 8
batch_size: 32
max_tokens: 4096
trust_remote_code: True
seed: 42
# --- データやモデルの配置設定
E5_path: "/data/hf_data/model/multilingual-e5-large"

# --- 質問データ生成パイプラインの設定
Seed_generation_method: "base"
Prompt_evolution: False
Prompt_evolution_times: 0
Number_of_stages_of_prompt_evolution: False
Data_retention_rate_after_diversity_cut: 50

# --- 回答データ生成パイプラインの設定
Answer_evolution: False
Answer_evolution_times: 0
Number_of_stages_of_answer_evolution: False

# --- 長考モデルの回答の設定
Using_think_models_for_answer: False
Thinking_separation_evolution: False
Number_of_thought_evolutions: 1

# --- 回答データキュレーションの設定
Data_curation: False

# --- 出力先などに関する設定
data_folda_path: "./data"
Save_temporary_data: True
Number_of_questions_generated: 500
output_path: "./data"